{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workbook 6: Supervised Machine Learning\n",
    "\n",
    "Overview of activities and objectives of this workbook:\n",
    "\n",
    "1. The first part of this workbook will introduce the K Nearest Neighbour algorithm for supervised learning.\n",
    "    - We will use the Iris dataset we introduced in the previous week.\n",
    "    - You are provided code for 1NN (1 nearest neighbour) and will extend this, using generative AI, to KNN (k nearest neighbours).\n",
    "    - We will also introduce several common Sklearn functions for splitting data into training and test sets, and visualising the results of classifiers.\n",
    "\n",
    "2. The second part of this workbook will introduce Decision Trees, another supervised learning algorithm.\n",
    "    - We will use the Sklearn implementation and explore how to limit tree growth (number/depth of branches) to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div>\n",
    "\n",
    "# Part 1: K nearest neighbours (KNN) \n",
    "\n",
    "## Loading the Iris dataset<img src=\"figures/Iris-image.png\" style=\"float:right;width:150;height:150\" alt=\"An image of an Iris flower with the petals and sepal labelled.\">\n",
    "\n",
    "First we will load the Iris data. This is a classic Machine Learning Data set which contains:\n",
    "- 4 measurements (features): sepal and petal width and length\n",
    "- 50 examples from each sub-species for iris flowers (so, 150 total)\n",
    "- 3 class labels: Iris-Virginica, Iris-Setosa or Iris-Versicolor\n",
    "\n",
    "The next cell to imports some useful libraries and then loads the iris dataset into two arrays:\n",
    "- <code>irisX</code> (the features - 150 rows x 4 columns)\n",
    "- <code>irisy</code> (the class labels - For the purpose of this tutorial we are going to ignore the fact that we are provided with class labels)\n",
    "- We'll also make a list of the <code>feature_names</code> so we can use them to label our plots.\n",
    "- Then we'll make a scatter plot to visualise the data.\n",
    "   - Reusing a useful function from week 5\n",
    "\n",
    "**Run the cell below to load and visualise the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mweek6_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mw6utils\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_iris\n",
      "File \u001b[1;32mc:\\Users\\USER\\Downloads\\Artificial_Intelligence_1-main\\Artificial_Intelligence_1-main\\Learning_Materials\\week_6\\week6_utils.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meuclidean_distance\u001b[39m(a,b):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import week6_utils as w6utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris data\n",
    "iris_data = load_iris(return_X_y=False)\n",
    "# Extract the data and labels, feature names, and label names\n",
    "irisX = iris_data.data\n",
    "irisy = iris_data.target\n",
    "feature_names = iris_data.feature_names\n",
    "label_names = iris_data.target_names\n",
    "\n",
    "print(f\"Iris has {irisX.shape[0]} samples and {irisX.shape[1]} features: {feature_names}\")\n",
    "print(f\"Iris has 3 classes: {label_names}\")\n",
    "\n",
    "# Create a scatter plot of all the Iris data\n",
    "w6utils.show_scatterplot_matrix(irisX, irisy, feature_names, \"Iris Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:3px\"></div>\n",
    "\n",
    "## Implementing K-Nearest Neighbours (KNN)\n",
    "\n",
    "Implementations of supervised ML algorithms typically have two functions `fit()` and `predict()`.\n",
    "\n",
    "**'Fitting'** -  means building/training the model with the training data, so that it (hopefully) makes good predictions.\n",
    "   - For  most ML algorithms **`fitting' is a search process**:\n",
    "      - search space <=> combinations of values for parameters that define a model's behaviour.\n",
    "      - quality measure <=> accuracy of predictions for training set (and maybe complexity)\n",
    "  - **For KNN**  fitting just means storing the training data\n",
    "     - there are sophisticated methods which *search* for the best subset of data to use, but we won't worry about them.\n",
    "\n",
    "**Predicting** - means using the model to make predictions e.g. the class of a particular data example.\n",
    "\n",
    "For KNN predicting the label of a new example from the test set:\n",
    "1. Measure distance to example point from every member of the training set.\n",
    "2. Find the K Nearest Neighbours.  \n",
    "   - In other words, the K members of the training set with the smallest distances (*calculated in step 1*)\n",
    "3. Count the labels of those K training items and return the most common one as the predicted label.\n",
    "\n",
    "Below is a figure illustrating the start and first two steps of process.  \n",
    "It is followed by a code cell with a simple implementation of a class for 1-Nearest neighbours i.e. only consider the 1 closest neighbour. \n",
    "\n",
    "<b>Read through the code  to get a sense for how it implements the algorithm.</b><br>\n",
    "Your tutor will discuss it with you in the lab sessions.\n",
    "<img src=\"figures/kNN-steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "<h3> Calculating straight-line distances in Python</h3>\n",
    "<p> At school you will have been taught <em>Pythagoras' Theorem<br>\n",
    "<em>The square of the hypotenuse (long side) of a right-angled triangle is equal to the sum of the square of the other two sides.</em>\n",
    "</p>\n",
    "<p>The image above has two dimensions at 90 degrees to each other: <em>x</em> (side to side) and <em>y</em> up and down.<br>\n",
    "    If we take any two points <em>a</em> and <em>b</em>, we can create a triangle  where:<ul>\n",
    "<li>the short-sides are the differences between <em>a</em> and <em>b</em> on the <em>x</em> and <em>y</em> dimensions</li>\n",
    "<li> the long side is the straight line between them (shown as dashed line in the middle image)</li>\n",
    "<li> Pythagoras' Theorem tells us how to calculate the distnacne between them! </li>\n",
    "    </ul>\n",
    "<p>In the general case of <em>n</em> dimensions this is known as the <em>Euclidean Distance</em> and is calculated in  the same way.<br>\n",
    "    \n",
    "Dist(a,b) = sqrt ( (a<sub>1</sub> - b<sub>1</sub>)<sup>2</sup>+ (a<sub>2</sub> - b<sub>2</sub>)<sup>2</sup> + ... + (a<sub>n</sub> - b<sub>n</sub>)<sup>2</sup>) \n",
    "</p>    \n",
    "\n",
    "<p> <b>This function is so common in all forms of computing that almost every language has a highly optimised version.</b> <br>\n",
    "In Python we wil use numpy's <code>np.linalg.norm</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "<h3>Enumerating lists with python</h3>\n",
    "<p>\n",
    "Quite often we may want to go through every item in a list and know:\n",
    "<ul>\n",
    "    <li> What the item is</li>\n",
    "    <li> what position it is in (index)</li>\n",
    "</ul>\n",
    "We can do this in fewer lines of code by using python's built-in <code>enumerate</code> function.<br>\n",
    "For example, this code snippet:<br>\n",
    "<pre><code>my_list = ('a', 'b', 'c')\n",
    "for idx, name in enumerate(my_list):\n",
    "    print(idx , name)</code></pre>\n",
    "produces the output:<br>\n",
    "0 a<br>\n",
    "1 b<br>\n",
    "2 c<br>\n",
    "</div>\n",
    "\n",
    "**Run the cell below** to define the class  for 1-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "class Simple1NNClassifier(ClassifierMixin):\n",
    "    \"\"\" Simple example class for 1-Nearest Neighbours algorithm.\n",
    "    Assumes numpy is imported as np and uses Euclidean distance\n",
    "    \"\"\"    \n",
    "    def dist_a_b(self, a:np.array, b:np.array)->float:\n",
    "        \"\"\" Euclidean distance between same-size vectors a and b\"\"\"\n",
    "        assert a.shape==b.shape, 'vectors not same size calculating distance'\n",
    "        return np.linalg.norm(a-b) \n",
    "    \n",
    "    def fit(self, x:np.ndarray, y:np.array):\n",
    "        \"\"\" just stores the data for k-nearest neighbour\"\"\"\n",
    "        self.model_x = x\n",
    "        self.model_y = y\n",
    "        self.is_fitted_=True\n",
    "        self.classes_=np.unique(y)\n",
    "        \n",
    "    def predict(self, new_items:np.ndarray):\n",
    "        \"\"\" makes predictions for an array of new items\"\"\"\n",
    "\n",
    "        # Get the number of new  and  stored items from their shapes\n",
    "        num_new = new_items.shape[0]\n",
    "        num_stored = self.model_x.shape[0]\n",
    "        \n",
    "        # Create empty array to store predictions\n",
    "        y_pred = np.zeros(num_new, dtype=int)\n",
    "        \n",
    "        # Create array of distances: one row(column) for each new(stored) item\n",
    "        distances = np.zeros((num_new, num_stored))\n",
    "        for row, new_item in enumerate(new_items):\n",
    "            for col, stored_example in enumerate(self.model_x):\n",
    "                distances[row][col]= self.dist_a_b(new_item, stored_example)\n",
    "\n",
    "        # Make predictions  for each new example in turn\n",
    "        for item_idx in range(num_new):\n",
    "            y_pred[item_idx] = self.predict_one(item_idx, distances)\n",
    "        \n",
    "        # Return the predictions\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_one(self, item_idx:int, distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        This version is just for 1 Nearest Neighbour\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx: int\n",
    "            index of item to make prediction for - i.e. idx of row in distances matrix\n",
    "\n",
    "        distances: numpy ndarray\n",
    "            array of distances between new items (rows) and training set records (columns)\n",
    "        \"\"\"\n",
    "        # We're going to use numpy's argmin() method - we saw this in week 1\n",
    "        # - We give it the row corresponding to the item to predict\n",
    "        # - It returns the index of column with lowest value in that row\n",
    "        idx_of_nearest_neighbour = np.argmin(distances[item_idx])\n",
    "\n",
    "        # Return the predicted class of the nearest neighbour\n",
    "        return self.model_y[ idx_of_nearest_neighbour]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:3px\"></div><br>\n",
    "\n",
    "## Applying 1NN to the Iris data\n",
    "\n",
    "We'll use the simple 1NN (K=1) classifier defined above and train and test on the Iris data we loaded previously.\n",
    "\n",
    "The next few cells demonstrate how to do this using some of Sklearn functions/classes.\n",
    "\n",
    "**Steps 1-5 are standard parts of all  supervised Machine Learning workflows**\n",
    "\n",
    "### Step 1: Split the data into train and test sets\n",
    "- `test_size` argument specifies how much of the data to keep back for testing (0.33 for the iris data is 50 for testing and 100 for training).\n",
    "- `stratify` argument makes sure our data has the same proportions of classes in train and test set (1:1:1 for the iris data set as this is *balanced*).\n",
    "\n",
    "**Run the cell below** to split the data **fairly** into train and test. It will print the size of the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make train/test split of datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy)\n",
    "\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an instance of the model class then *fit*  it to the training data\n",
    "**Run the cell below**, it will not produce any output. *Some* ML algorithms with longer training times (e.g. artificial neural networks) may produce `progress indicators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_1NN_model = Simple1NNClassifier()\n",
    "my_1NN_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the trained model's performance on unseen test data\n",
    "**Run the three cells below** after making sure you understand what they are doing.  \n",
    "- The first is a reusable function with explicit code that outputs predictions, whether they are the same as the true values, and then creports the proportion correct.  \n",
    "- The second cell creates a shorter version in a **function that you can re-use.**\n",
    "- the third then calls this function for your classifier and test data- it should give the same output as the first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "predictions = my_1NN_model.predict(test_x)\n",
    "print(f'Predictions are:\\n {predictions}')\n",
    "\n",
    "# Make array of True/False values for each prediction\n",
    "# By comparing the predictions to the actual label values\n",
    "print(f'Prediction matches to actual label values are:\\n{test_y==predictions}')\n",
    "\n",
    "# Calculate the accuracy (as a percentage)\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(classifier,dataset,truelabels)->float:\n",
    "    \"\"\"returns the percentage of correct predictions made by a classifier on a dataset\"\"\"\n",
    "    # data and labels must be same size\n",
    "    assert dataset.shape[0]== truelabels.shape[0], 'dataset and labels are not same size'\n",
    "\n",
    "    try:\n",
    "        predictions= classifier.predict(dataset)\n",
    "        accuracy = 100 * (truelabels == predictions).sum() / truelabels.shape[0]\n",
    "        return accuracy\n",
    "    except Exception as e:\n",
    "        print(f' Had a problem running your code:\\n {e}')\n",
    "        return -1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2 = get_accuracy(my_1NN_model,test_x,test_y)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4: Visualise a confusion matrix\n",
    "A confusion matrix shows the counts of predictions vs the true label for each example. So correct predictions appear on the diagonal.\n",
    "\n",
    "This is often more useful than just calculating accuracy because **it shows where the classifier is making mistakes**.\n",
    "\n",
    "The Iris data is quite easy, so most models will make correct predictions for *setosa*  \n",
    "but misclassify one or two instances of *versicolor* and *virginica*.\n",
    "\n",
    "**Run the cell below** to create confusion matrix using tools from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names)\n",
    "disp.figure_.set_size_inches(5, 5)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualise the decision boundary\n",
    "\n",
    "The decision boundary shows how, within the 'decision space' the model is making predictions.\n",
    "\n",
    "This can be useful to compare how different algorithms make decisions but is hard for most people to recognise in more than 2 dimensions.\n",
    "\n",
    "So we will quickly train a model using a 2D version of the iris data set (just the petal measurements).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 1: Make data using numpy slicing to just select the first two columns for every row\n",
    "petal_trainx = train_x[:, :2]\n",
    "petal_testx = test_x[:, :2]\n",
    "\n",
    "# 2: Instantiate and fit model to data\n",
    "#my_1NN_2d = KNeighborsClassifier(n_neighbors=1)\n",
    "my_1NN_2d = Simple1NNClassifier()\n",
    "my_1NN_2d.fit(petal_trainx, train_y)\n",
    "\n",
    "# 3: Make predictions, score them \n",
    "accuracy = get_accuracy(my_1NN_2d,petal_testx,test_y)\n",
    "num_errors= len(test_y)*(100-accuracy) / 100\n",
    "print(f\"Overall Accuracy in 2D = {accuracy}%, \"\n",
    "      f\"model makes {num_errors} mistakes on test data.\"\n",
    "      )\n",
    "\n",
    "# 4: Create the decision boundary learned by model from training data\n",
    "disp = DecisionBoundaryDisplay.from_estimator(my_1NN_2d, petal_trainx, alpha=0.5)\n",
    "\n",
    "# # Show where the test is within the decision boundary\n",
    "disp.ax_.scatter(petal_testx[:, 0], petal_testx[:, 1], c=test_y, edgecolor='black', label='test')\n",
    "_= disp.ax_.set_title(\"1NN decision boundary on petal features\")\n",
    "disp.figure_.set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 1: Complete K-nearest neighbours with GenAI</h2>\n",
    "    <p>The <code>Simple1NNClassifier()</code> class above only implements KNN for K=1, i.e. only considers the single closest neighbour when making predictions. </p>\n",
    "    <p>In this activity you will extend (via inheritance) the <code>Simple1NNClassifier()</code> into a full KNN classifier. </br>\n",
    "    In other words, to make a prediction for a new example, it needs to find the most common class amongst the  the K nearest neighbours.</p>\n",
    "    <p>You need to complete the <code>SimpleKNNClassifier()</code> class below by:</p>\n",
    "    <ol>\n",
    "        <li>Over-riding the <code>def __init__()</code> to take one parameter <code>K</code>: the number of neighbours to consider</li>\n",
    "        <li>Over-riding the <code>predict_one()</code> method so that it:\n",
    "            <ol>\n",
    "                <li>Finds the indexes of the <code>self.K</code> nearest neighbours from the <code>distances</code> array.<li>Stores the labels of these neighbours.</li>\n",
    "                <li>Finds the most common label within the neighbours.</li>\n",
    "                <li>Returns the most common label as the prediction for the new example.</li>\n",
    "            </ol>\n",
    "        </li>\n",
    "    </ol>\n",
    "    There are lots of different ways to implement this functionality in Python. For this activity it is suggested you use <b>Generative AI</b> tools to complete the <code>predict_one()</code> method (unless you prefer to code it yourself!). <br><br>\n",
    "    The next cell below lets you test your KNN implementation with K=1 on the Iris data. You should find it reaches the same accuracy as the <code>Simple1NNClassifier()</code>. Then in the following activity we will think about how to test your implementation to verify it is correct.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Hints:</b> \n",
    "    <ul>\n",
    "        <li>If you choose to use a GenAI tool with a text/web interface, like ChatGPT or Gemini, you should think carefully about how you construct the prompt. You aren't asking for a full KNN implementation, only a function that makes predictions for 1 new example, so:\n",
    "            <ul>\n",
    "                <li>Specify the parameters the function takes.</li>\n",
    "                <li>What the output should be.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>If you choose to use Github Copilot (or similar), you might find it easier to start typing the function definition (<code>def predict_one()</code>) and see if it provides a completion suggestion.\n",
    "            <ul><li>Or, see if you can get copilot to help you complete each step above individually.</li></ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Write your implementation where indicated in the cell below, then run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleKNNClassifier(Simple1NNClassifier):\n",
    "    \"\"\"Complete this class to produce a KNN classifier\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor for the KNN classifier\n",
    "        you will need to change the function signature to expect and store a parameter K\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "\n",
    "    def predict_one(self, item_idx:int, distances:np.ndarray):\n",
    "        \"\"\" makes a class prediction for a single new item\n",
    "        This version is for K Nearest Neighbour\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item_idx: int\n",
    "            index of item to make prediction for - i.e. idx of row in distances matrix\n",
    "\n",
    "        distances: numpy ndarray\n",
    "            array of distances between new items (rows) and training set records (columns)\n",
    "        \"\"\"\n",
    "        # ====> insert your code below here\n",
    "\n",
    "        #you will need to delete the following line for your code to run\n",
    "        raise NotImplementedError(\"Complete the function\")\n",
    "        \n",
    "        # <==== insert your code above here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the cell** below to do **one** test of your implementation.  \n",
    "- All this tests does it see if your code works.\n",
    "- Because the train / test set are the same, it *should* get the same accuracy as you saw above\n",
    "when you ran the Simple1NNClassifier.\n",
    "- But this is just testing that it gets the same *number* of predictions correct, which is quite a crude test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with K=1\n",
    "my_KNN_model = SimpleKNNClassifier(K=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "my_KNN_model.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions for test data\n",
    "predictions = my_KNN_model.predict(test_x)\n",
    "print(f'Predictions are:\\n {predictions}')\n",
    "\n",
    "# Calculate the accuracy\n",
    "test_accuracy=get_accuracy(my_KNN_model,test_x,test_y)\n",
    "print(f\"\\nOverall Accuracy = {test_accuracy} %\")\n",
    "\n",
    "my_1NN_model = Simple1NNClassifier()\n",
    "my_1NN_model.fit(train_x,train_y)\n",
    "test_accuracy=get_accuracy(my_1NN_model,test_x,test_y)\n",
    "print(f'1NN test accuracy {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 2: Evaluating KNN and GenAI</h2>\n",
    "    Generative AI is very powerful and can save us a lot of time by helping to write code. However, you <b>should not blindly trust any code (or any thing) GenAI creates</b>. GenAI doesn't really <i>'understand'</i> what we are asking it to do. It is simply generating text/code/images based on its <i>prediction</i> of what is most likely correct. This does not mean it is correct, and sometimes it will <i>'hallucinate'</i>, i.e. make stuff up!<br>\n",
    "    So you should always <i>verify</i> anything you ask GenAI to create. For code this should come from your understanding of the code it has generated and the algorithm. But for a more concrete test we can also write some test cases to check the output is correct.<br><br>\n",
    "    Your <code>SimpleKNNClassifier()</code> with <code>K=1</code> should reach the same accuracy as the previous <code>Simple1NNClassifier()</code> did. <br>But we should check it works for other values of K and also on other data.<br><br>\n",
    "    Complete the following 3 cells to implement 3 different test cases and evaluate your GenAI version of KNN:\n",
    "    <ol>\n",
    "        <li>Compare with the Sklearn implementation of KNN on Iris data. In this case, we know the Sklearn KNN classifier is correct. So we can compare the Sklearn KNN predictions with our own implementation:\n",
    "            <ul>\n",
    "                <li>Create an instance of <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">Sklearn kNeighboursClassifier</a> and run it on the Iris data to make predictions.</li>\n",
    "                <li>Create an instance of your <code>SimpleKNNClassifier()</code> and run it on the Iris data to make predictions.</li>\n",
    "                <li>Compare Sklearn and your classifier predictions accuracy on different values of K.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Compare with the Sklearn implementation of KNN on random data. This is the same as step 1, except instead of Iris data we will randomly generate some.\n",
    "            <ul>\n",
    "                <li>Use the <a href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.make_classification.html\">Sklearn make_classification()</a> function to create a dataset with  500 samples, 2 features and 4 classes. </li>\n",
    "                <li>Compare Sklearn and your classifier as before.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Construct simple 2D data to test KNN. We can manually construct data that will give different predictions for different values of K, e.g. K=1 predict 0, K=3 predict 1, K=5 predict 0 etc.\n",
    "            <ul>\n",
    "                <li>Create two 2D arrays of 5 examples (data points) each. One array for class 0 and one array for class 1.</li>\n",
    "                <li>Test on a single example. Suggest you use <code>[4.1, 4.1]</code> but you can use another if you prefer!</li>\n",
    "                <li>Compare Sklearn and your classifier as before. The predictions should be different for increasing values of K, e.g. <code>[1, 0, 1, 0, 1]</code> for k values <code>[1, 3, 5, 7, 9]</code> </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\">\n",
    "<h3>Hints</h3>\n",
    "<ul>\n",
    "<li> You have been given nearly all the code you need</li>\n",
    "<li>It makes your life much simpler if you re-use the variable names <code>train_x, test_x,train_y,test_y</code>for each dataset (test)</li>\n",
    "<li>For each test the main loop ( <code>for K in ... </code>) needs to:\n",
    "<ol>\n",
    "<li>Create an instance of KNeighborsClassifier with <code>n_neighbors=K</code></li>\n",
    "<li>Fit the classifier to your training data</li>\n",
    "<li> Store the classifier's predictions  for the test data in <code>skl_knn_pred</code></li>\n",
    "<li>call the <code>get_accuracy()</code> function we defined above  and store the results in <code>skl_acc</code></li>\n",
    "<li> repeat with your SimpleKNNClassifier</code> class but store the predictions and accuracy in <code>my_knn_pred</code> and <code>my_acc</code></li>\n",
    "    </ol></li>\n",
    "<li> The link above shows how to make the synthetic data. You need to provide values to over-ride the defaults for <code>n_samples, n_features, n_classes</code></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Compare with the Sklearn implementation of KNN on Iris data.  \n",
    "**Write your implementation where indicated in the cell below** then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Reload and use a fresh split of the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "irisX,irisy=load_iris(return_X_y=True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy)\n",
    "\n",
    "\n",
    "# Test different values of K\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    # ====> insert your code below here\n",
    "\n",
    "    # <==== insert your code above here\n",
    "    print(f\"Accuracy of Sklearn model is {skl_acc:.2f}%, your model is {my_acc:.2f}%\")\n",
    "    assert skl_acc == my_acc, f\"Accuracy of Sklearn model is not the same as your model for K={K}\"\n",
    "\n",
    "    # Compare individual predictions\n",
    "    assert (skl_knn_pred == my_knn_pred).all(), \"Predictions are not the same for K={K}\"\n",
    "    print(f\"Sklearn and your model make the same predictions for K={K}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2: Compare with the Sklearn implementation of KNN on random data.   \n",
    "**Write your implementation where indicated in the cell below** then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create a random dataset with 500 samples, 2 features and 4 classes\n",
    "\n",
    "# ====> insert your code below here\n",
    "\n",
    "# <==== insert your code above here\n",
    "\n",
    "# Visualise the data\n",
    "fig, an = plt.subplots(figsize=(8, 8))\n",
    "an.scatter(X[:, 0], X[:, 1], c=y)\n",
    "an.set_title(\"Random dataset with 4 classes\")\n",
    "plt.show()\n",
    "\n",
    "# Create a train/test split of the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.33, stratify=y)\n",
    "\n",
    "# Test different values of K\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    # ====> insert your code below here\n",
    " \n",
    "\n",
    "    # <==== insert your code above here\n",
    "    print(f\"Accuracy of Sklearn model is {skl_acc:.2f}%, your model is {my_acc:.2f}%\")\n",
    "    assert skl_acc == my_acc, f\"Accuracy of Sklearn model is not the same asyour model for K={K}\"\n",
    "\n",
    "    # Compare individual predictions\n",
    "    assert (skl_knn_pred == my_knn_pred).all(), \"Predictions are not the same for K={K}\"\n",
    "    print(f\"Sklearn and your model make the same predictions for K={K}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3: Construct simple 2D data to test KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate synthetic 2D data with 5 examples in each class\n",
    "# ====> insert your code below here\n",
    "\n",
    "# <==== insert your code above here\n",
    "\n",
    "# Combine data and labels\n",
    "X = np.vstack((class_0, class_1))\n",
    "y = [0] * len(class_0) + [1] * len(class_1)\n",
    "\n",
    "# Test point\n",
    "test_y = np.array([[4.1, 4.1]])\n",
    "\n",
    "# Visualise the data\n",
    "fig, an = plt.subplots(figsize=(8, 8))\n",
    "an.scatter(X[:, 0], X[:, 1], c=y)\n",
    "an.scatter(test_y[:, 0], test_y[:, 1], c='red', marker='x', s=100)\n",
    "an.set_title(\"Synthetic dataset with 2 classes\")\n",
    "plt.show()\n",
    "\n",
    "# Test different values of K\n",
    "predictions = []\n",
    "for K in [1, 3, 5, 7, 9]:\n",
    "    # ====> insert your code below here\n",
    "\n",
    "\n",
    "    # <==== insert your code above here\n",
    "    assert skl_knn_pred == my_knn_pred, f\"Prediction is not the same for K={K}\"\n",
    "    predictions.append(my_knn_pred[0].item())\n",
    "    print(f\"Sklearn and your model make the same predictions {my_knn_pred[0]} and {skl_knn_pred[0]} for K={K}\\n\")\n",
    "\n",
    "# Check the predictions were different for different values of K\n",
    "assert predictions == [1, 0, 1, 0, 1], \"Predictions are not as expected\"\n",
    "print(f\"All predictions are as expected {predictions} for different values of K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 3: Experiment with KNN</h2>\n",
    "Now that you have established that your implementation of KNN is correct, lets explore a few things that might effect its output predictions.\n",
    "\n",
    "Creating different splits of the Iris data using  Sklearn's <code>train_test_split()</code> function:\n",
    "    <ul>\n",
    "        <li>Try changing the <code>random_state</code> parameter.</li>\n",
    "        <li>Try removing the <code>stratify</code> parameter.</li>\n",
    "        <li>Try changing the <code>test_size</code> parameter.</li>\n",
    "        <li>Does it reach the same accuracy/predictions as before?</li>\n",
    "        <li>If these are not the same, can you explain why not?</li>\n",
    "    </ul>\n",
    "\n",
    "In Machine Learning we talk about algorithms having <b>hyper-parameters</b> that control their behaviour. For KNN <code>K</code> is a <i>hyperparamter</i>. Try running and evaluating KNN with K = {3, 5, 7, 9}:\n",
    "    <ul>\n",
    "        <li>Make <b>qualitative</b> judgements: how does the decision surface change? (you might need to select 2 features as before).</li>\n",
    "        <li>Make <b>quantitative</b> judgements:  how does the confusion matrix change?</li>\n",
    "        <li>What value for the hyper-parameter <b>K</b> gives the best accuracy on the <b>train</b> set?</li>\n",
    "        <li>What value for the hyper-parameter <b>K</b> gives the best accuracy on the <b>test</b> set?</li>\n",
    "        <li>If these are not the same, can you explain why not?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make train/test split of datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy, random_state=42)\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')\n",
    "\n",
    "# Set K\n",
    "K = 3\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "knn = SimpleKNNClassifier(K=K)\n",
    "knn.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy for K={K} is {accuracy:.2f}%\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names)\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.plot()\n",
    "\n",
    "# Create the decision boundary learned by model from training data\n",
    "trainx_2_features = train_x[:, :2]\n",
    "testx_2_features = test_x[:, :2]\n",
    "knn_2d = KNeighborsClassifier(n_neighbors=K).fit(trainx_2_features, train_y)\n",
    "disp = DecisionBoundaryDisplay.from_estimator(knn_2d, testx_2_features, alpha=0.5)\n",
    "disp.ax_.scatter(petal_testx[:, 0], petal_testx[:, 1], c=test_y, edgecolor='black', label='test')\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:10px\"></div><br>\n",
    "\n",
    "# Part 2: Decision Trees\n",
    "\n",
    "In the lecture notebook we illustrated how the decision tree is created by dividing input space using a set of axis-parallel lines.\n",
    "\n",
    "The tree is 'grown' by:\n",
    "1. Start with single node that predicts majority class label.\n",
    "2. Loop over every leaf node:\n",
    "    - Measure (in some way) the \"information content\" of the data that arrives at that node.\n",
    "    - For each possible data split:\n",
    "        - measure and add the \"information content\" of the child nodes created by the split\n",
    "        - subtract information content of parent\n",
    "        - result is the *gain* in information content given by split\n",
    "        - update stored \"best split\" if appropriate\n",
    "    - If the  \"best\" split is above some threshold then change the leaf node to an interior node with the *best* condition.\n",
    "    - If <i>termination criteria</i> not met goto step 2.\n",
    "\n",
    "The following cells demonstrate how to create, train and evaluate a Decision Tree using Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/test split of datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy)\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create an instance of the model class then *fit*  it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_dt = DecisionTreeClassifier()\n",
    "my_dt.fit(train_x, train_y)\n",
    "\n",
    "# Show the decision tree\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "tree.plot_tree(my_dt, feature_names=feature_names, class_names=label_names, filled=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the trained model's performance on unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "predictions = my_dt.predict(test_x)\n",
    "print(f'Predictions are:\\n {predictions}')\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"\\nOverall Accuracy = {accuracy:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualise a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names)\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualise the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionBoundaryDisplay\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Make data using numpy slicing to just pull the last two columns for every row\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m petal_trainx \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_x\u001b[49m[:, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      5\u001b[0m petal_testx \u001b[38;5;241m=\u001b[39m test_x[:, :\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Instantiate and fit model to data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Make data using numpy slicing to just pull the last two columns for every row\n",
    "petal_trainx = train_x[:, :2]\n",
    "petal_testx = test_x[:, :2]\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_dt_2d = DecisionTreeClassifier()\n",
    "my_dt_2d.fit(petal_trainx, train_y)\n",
    "\n",
    "# Make predictions, score them \n",
    "y_pred = my_dt_2d.predict(petal_testx)\n",
    "accuracy = 100 * ( test_y == y_pred).sum() / test_y.shape[0]\n",
    "num_errors= len(y_pred) - (test_y == y_pred).sum()\n",
    "print(f\"Overall Accuracy in 2D = {accuracy}%, model makes {num_errors} mistakes\")\n",
    "\n",
    "# Create the decision boundary learned by model from training data\n",
    "disp = DecisionBoundaryDisplay.from_estimator(my_dt_2d, petal_trainx, alpha=0.5)\n",
    "\n",
    "# # Show where the test is within the decision boundary\n",
    "disp.ax_.scatter(petal_testx[:, 0], petal_testx[:, 1], c=test_y, edgecolor='black', label='test')\n",
    "_= disp.ax_.set_title(\"Decision Tree decision boundary on petal features\")\n",
    "disp.figure_.set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:black;width:100%;height:5px\"></div><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\"><h2>Activity 4: Experiment with Decision Trees</h2>\n",
    "We often want to control how we learn a model (in this case, grow a tree) to avoid a phenomenon call <i>over-fitting</i>. This is where the model is capturing fine-details of the training set and so failing to generalise from the training set to the real world.\n",
    "\n",
    "The aim of this activity is for you to experiment with what happens when you change two <b>hyper-parameters</b> that affect how big and complex the tree is allowed to get.\n",
    "<ul>\n",
    "    <li><code>max_depth</code>: default is None</li>\n",
    "    <li><code>min_samples_leaf</code>: default value is 1</li>\n",
    "</ul>\n",
    "Experiment with the Iris data set we loaded earlier to see if you can work out what each of these hyper-parameters does, and how it affects the tree. Try running and evaluating Decision Trees with different values for <code>max_depth</code> = {None, 1, 3, 5} and <code>min_samples_leaf</code> = {1, 3, 5}:\n",
    "    <ul>\n",
    "        <li>Do some combinations result in bigger differences between accuracy on the train / test sets?</li>\n",
    "        <li>Is there a combination of hyper-parameter values that means you consistently get similar trees?</li>\n",
    "        <li>What is a good way of judging 'similarity?</li>\n",
    "        <li>Do different train/test splits affect what tree you get?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'irisX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make train/test split of datasets\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_x, test_x, train_y, test_y \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mirisX\u001b[49m, irisy, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.33\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mirisy, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining set has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples, test set has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Experiment with changing these values\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'irisX' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make train/test split of datasets\n",
    "train_x, test_x, train_y, test_y = train_test_split(irisX, irisy, test_size=0.33, stratify=irisy, random_state=42)\n",
    "print(f'Training set has {train_x.shape[0]} examples, test set has {test_x.shape[0]} examples')\n",
    "\n",
    "# Experiment with changing these values\n",
    "depth = None  # Try None 1, 3, 5\n",
    "min_leaf = 3  # Try 1, 3, 5\n",
    "\n",
    "# Instantiate and fit model to data\n",
    "my_dt = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_leaf)\n",
    "my_dt.fit(train_x, train_y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = my_dt.predict(test_x)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (test_y == predictions).sum() / test_y.shape[0]\n",
    "print(f\"Overall Accuracy for depth={depth} and min_leaf={min_leaf} is {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "# Plot the confusion matrix and the tree side by side\n",
    "fig, ax= plt.subplots(ncols=2, figsize=(8, 8)) \n",
    "cm = ConfusionMatrixDisplay.from_predictions(test_y, predictions, display_labels=label_names, colorbar=False, ax=ax[0])\n",
    "_ = tree.plot_tree(my_dt, feature_names=feature_names, class_names=label_names, filled=True, ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\"><b>Save and close Jupyter:</b>\n",
    "    <ol>\n",
    "        <li>Use the jupyterlab functions to download your work (ask your tutor if you need help with this) and save it somewhere sensible so you can find it easily.</li>\n",
    "        <li>Shutdown the notebook when you have finished with this tutorial (menu->file->close and shutdown notebook)</li>\n",
    "    </ol>\n",
    "</div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
